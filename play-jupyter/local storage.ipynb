{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFN-MILANO-ATLASC_LOCALGROUPDISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install plotly --upgrade\n",
    "from plotly import __version__\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, iplot\n",
    "import plotly.express as px\n",
    "init_notebook_mode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# WARNING for pandas >=0.17: https://github.com/pydata/pandas/issues/11786\n",
    "import logging\n",
    "import datetime\n",
    "import urllib\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['image.cmap'] = 'coolwarm'  # quite good colormap, should avoid rainbow problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: solve this\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gb = 1024. ** 3\n",
    "Tb = 1024. ** 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(rse, date, **kwargs):\n",
    "    datestr = date.strftime('%d-%m-%Y')\n",
    "    url = \"https://rucio-hadoop.cern.ch/consistency_datasets?rse=%s&date=%s\" % (rse, datestr)\n",
    "    print(url)\n",
    "    return to_pandas(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pandas(filename):\n",
    "    def conv(s):\n",
    "        s = set(s.split(\",\"))\n",
    "        to_remove = \"panda\", \"root\"\n",
    "        for name in to_remove:\n",
    "            if name in s:\n",
    "                s.remove(name)\n",
    "        s = \",\".join(s)\n",
    "        return s\n",
    "\n",
    "    try:\n",
    "        names = (\n",
    "            \"RSE\",\n",
    "            \"scope\",\n",
    "            \"name\",\n",
    "            \"owner\",\n",
    "            \"size\",\n",
    "            \"creation_date\",\n",
    "            \"last_accessed_date\",\n",
    "            \"rule_id\",\n",
    "            \"n_replicas\",\n",
    "            \"update_date\",\n",
    "        )\n",
    "        data = pd.read_csv(\n",
    "            filename,\n",
    "            sep=\"\\t\",\n",
    "            header=None,\n",
    "            parse_dates=[\"creation_date\", \"last_accessed_date\", \"update_date\"],\n",
    "            date_parser=lambda _: pd.to_datetime(float(_), unit=\"ms\"),\n",
    "            converters={\"owner\": conv},\n",
    "            names=names,\n",
    "        )\n",
    "\n",
    "    except Exception as ex:\n",
    "        if not isinstance(ex, urllib.error.HTTPError):\n",
    "            print(\n",
    "                (\"cannot parse file from %s: %s\" % (filename, str(ex.msg)))\n",
    "            )  # pylint: disable=E1101\n",
    "\n",
    "        raise\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "milano_rse = \"INFN-MILANO-ATLASC_LOCALGROUPDISK\"\n",
    "yesterday = datetime.datetime.now() - datetime.timedelta(days=1)\n",
    "date = yesterday\n",
    "\n",
    "data = get_data(milano_rse, date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = get_data(milano_rse, datetime.datetime.now() - datetime.timedelta(days=200))\n",
    "data = data.set_index([\"RSE\", \"scope\", \"name\"])\n",
    "\n",
    "data2 = data2.set_index([\"RSE\", \"scope\", \"name\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.DataFrame({\"RSE\": [], \"scope\": [], \"name\": [], \"owner\": [], \"size\": [], \"creation_date\": [], \"last_accessed_date\": [], \"rule_id\": [], \"n_replicas\": [], \"update_date\": []})\n",
    "full_df = full_df.set_index([\"RSE\", \"scope\", \"name\"])\n",
    "\n",
    "\n",
    "index_missing = data.index.difference(full_df.index)\n",
    "index_common = data.index.intersection(full_df.index)\n",
    "full_df = pd.concat([full_df, data.loc[index_missing]]).sort_index()\n",
    "full_df.loc[index_common] = data.loc[index_common]\n",
    "\n",
    "index_missing = data2.index.difference(full_df.index)\n",
    "index_common = data2.index.intersection(full_df.index)\n",
    "full_df = pd.concat([full_df, data2.loc[index_missing]]).sort_index()\n",
    "full_df.loc[index_common] = data2.loc[index_common]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2.loc[index_common].loc[(data.loc[index_common] != data2.loc[index_common])['creation_date'].values].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_common = data2.index.intersection(data.index)\n",
    "\n",
    "data.loc[index_common].loc[(data.loc[index_common] != data2.loc[index_common])['creation_date'].values]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print the first datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_json(\"data_all.json\", orient='records', date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.pytables import HDFStore\n",
    "store = HDFStore('store.h5')\n",
    "store[str(date)] = data\n",
    "store.close()\n",
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['owner'] == 'resconi']['size'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hierarchical_dataframe(df, levels, value_column, color_columns=None):\n",
    "    \"\"\"\n",
    "    Build a hierarchy of levels for Sunburst or Treemap charts.\n",
    "\n",
    "    Levels are given starting from the bottom to the top of the hierarchy,\n",
    "    ie the last level corresponds to the root.\n",
    "    \"\"\"\n",
    "    df_all_trees = pd.DataFrame(columns=['id', 'parent', 'value', 'color'])\n",
    "    for i, level in enumerate(levels):\n",
    "        df_tree = pd.DataFrame(columns=['id', 'parent', 'value', 'color'])\n",
    "        dfg = df.groupby(levels[i:]).sum()\n",
    "        dfg = dfg.reset_index()\n",
    "        df_tree['id'] = dfg[level].copy()\n",
    "        if i < len(levels) - 1:\n",
    "            df_tree['parent'] = dfg[levels[i+1]].copy()\n",
    "        else:\n",
    "            df_tree['parent'] = 'total'\n",
    "        df_tree['value'] = dfg[value_column]\n",
    "        #df_tree['color'] = dfg[color_columns[0]] / dfg[color_columns[1]]\n",
    "        df_all_trees = pd.concat([df_all_trees, df_tree], ignore_index=True)\n",
    "    total = pd.Series(dict(id='total', parent='',\n",
    "                              value=df[value_column].sum(),\n",
    "                              ))\n",
    "    df_all_trees = pd.concat([df_all_trees, total], ignore_index=True)\n",
    "    return df_all_trees\n",
    "\n",
    "df_2d = data.groupby(['scope', 'owner'])['size'].sum().sort_index().reset_index()\n",
    "\n",
    "\n",
    "fig = px.sunburst(\n",
    "    build_hierarchical_dataframe(data.reset_index(), ['scope', 'owner'], 'size'),\n",
    "    names='id',\n",
    "    parents='parent',\n",
    "    values='value',\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2d = data.groupby(['scope', 'owner'])['size'].sum().sort_index()\n",
    "fig = px.sunburst(df_2d.reset_index(), path=['owner', 'scope'], values='size')\n",
    "fig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = data['size'].sum() / Tb\n",
    "print(\"total usage = %.2f Tb\" % total)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Users sorted by usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_owner(data, by='owner'):\n",
    "    default_actions = {'owner':'count', 'life_days': 'mean', 'age_days': 'mean', 'last_accessed_days': 'mean', 'size': lambda x: np.sum(x) / Tb}\n",
    "    actions = {k: default_actions[k] for k in data.columns if k in default_actions}\n",
    "    result = data.groupby(by).agg(actions)\n",
    "    result = result.rename(columns={'owner': 'ndatasets'})\n",
    "    return result\n",
    "\n",
    "group_owner = group_by_owner(data)\n",
    "group_owner = group_owner.sort_values(['size'], ascending=False)\n",
    "\n",
    "group_owner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_scope = group_by_owner(data, 'scope')\n",
    "group_scope = group_scope.sort_values(['size'], ascending=False)\n",
    "\n",
    "group_scope"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Size of the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 1, figsize=(20, 7))\n",
    "\n",
    "median = np.median(data['size']) / Gb\n",
    "\n",
    "data_size_not_null_Gb = data[data['size'].notnull()]['size'] / Gb\n",
    "\n",
    "data_size_not_null_Gb.hist(bins=np.logspace(-6, 5, 50), ax=ax[0], log=True, histtype='stepfilled')\n",
    "data_size_not_null_Gb.hist(bins=200, range=(0, 10), ax=ax[1], histtype='stepfilled')\n",
    "data_size_not_null_Gb[data_size_not_null_Gb < .005].hist(bins=200, ax=ax[2], histtype='stepfilled')\n",
    "ax[0].set_xscale('log')\n",
    "ax[0].set_xlabel('dataset size [Gb]')\n",
    "ax[1].set_xlabel('dataset size [Gb]')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "display((data['size'] / Gb).describe())\n",
    "\n",
    "print(\"median dataset = %.2f GB\" % (median))\n",
    "print(\"fraction dataset < 1 GB = %.1f%%\" % (len(data[data['size'] / (1024.**3) < 1]) / float(len(data)) * 100.))\n",
    "print(\"fraction dataset < 100 MB = %.1f%%\" % (len(data[data['size'] / (1024.**2) < 100]) / float(len(data)) * 100.))\n",
    "\n",
    "print(\"5 smallest samples\")\n",
    "display(data[data['size'].notnull()].sort_values('size')[:5][['RSE', 'owner', 'name', 'size']])\n",
    "print(\"5 largest samples\")\n",
    "display(data[data['size'].notnull()].sort_values('size')[-5:][['RSE', 'owner', 'name', 'size']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 2, figsize=(18, 4))\n",
    "\n",
    "g = data.groupby('owner')['size']\n",
    "owners = data['owner'].unique()\n",
    "\n",
    "axs[0].hist([g.get_group(user).values / Gb for user in owners], bins=(np.logspace(-5, 3.1, 50)), stacked=True, fill=True, histtype='stepfilled', label=tuple(owners), density=True)\n",
    "axs[0].legend()\n",
    "axs[0].set_yscale('log')\n",
    "axs[0].set_xscale('log')\n",
    "axs[0].set_xlabel('size [Gb]')\n",
    "\n",
    "\n",
    "axs[1].hist([g.get_group(user).values / Gb for user in owners], bins=(np.logspace(-5, 3.1, 50)), stacked=True, fill=True, histtype='stepfilled', label=tuple(owners), density=True)\n",
    "axs[1].legend()\n",
    "axs[1].set_xscale('log')\n",
    "axs[1].set_xlabel('size [Gb]')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset age (when they are created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "data['age_days'].hist(bins=np.logspace(1, 4, 30), ax=ax[0], histtype='stepfilled')\n",
    "data['age_days'].hist(bins=30, ax=ax[1], histtype='stepfilled')\n",
    "\n",
    "ax[0].set_xscale('log')\n",
    "for a in ax: a.set_xlabel('age [days from today]')\n",
    "print(\"average age: %d days\" % data.age_days.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for owner, df_owner in data.groupby('owner'):\n",
    "    print(\"owner: %s\" % owner)\n",
    "    \n",
    "    display(df_owner.sort_values('age_days', ascending=False)[['name', 'creation_date', 'state']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset last access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "data['last_accessed_days'].hist(bins=np.logspace(1, 4, 30), ax=ax[0], histtype='stepfilled')\n",
    "data['last_accessed_days'].hist(bins=30, ax=ax[1], histtype='stepfilled')\n",
    "ax[0].set_xscale('log')\n",
    "for a in ax: a.set_xlabel('last accessed [days from today]')\n",
    "print(\"average last access: %d days\" % data.last_accessed_days.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Life (time between creation and last access)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "data['life_days'].hist(bins=np.logspace(1, 3, 30), ax=ax[0], histtype='stepfilled')\n",
    "data['life_days'].hist(bins=50, ax=ax[1], range=(0, 1000), histtype='stepfilled')\n",
    "ax[0].set_xscale('log')\n",
    "for a in ax: a.set_xlabel('last accessed - date creation [days]')\n",
    "\n",
    "print \"average life: %d days\" % data.life_days.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No correlation between size and age or last access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(20, 7))\n",
    "data.plot(kind='scatter', y='size', x='last_accessed_days', ax=ax[0], alpha=0.05)\n",
    "ax[0].set_yscale('log')\n",
    "\n",
    "data.plot(kind='scatter', y='size', x='age_days', ax=ax[1], alpha=0.05)\n",
    "ax[1].set_yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = data.corr()\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "p = ax.pcolormesh(np.nan_to_num(corr.values), vmin=-1, vmax=1)\n",
    "ax.set_yticklabels(corr.index)\n",
    "ax.set_xticklabels(corr.index, rotation=90)\n",
    "ax.set_yticks(np.arange(len(corr.index)) + 0.5)\n",
    "ax.set_xticks(np.arange(len(corr.index)) + 0.5)\n",
    "plt.colorbar(p)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "group_owner['size'].plot(kind='pie', autopct='%.0f', ax=ax[0])\n",
    "group_owner['size'].plot(kind='bar', ax=ax[1])\n",
    "ax[1].set_ylabel('size [Tb]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
    "group_owner['ndatasets'].plot(kind='pie', autopct='%.0f', ax=ax[0])\n",
    "group_owner['ndatasets'].plot(kind='bar', ax=ax[1])\n",
    "ax[1].set_ylabel('# datasets')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Size per dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some users have huge datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n",
    "ave_size = group_owner['size'] / group_owner['ndatasets'] * 1024  # Gb\n",
    "ave_size.sort_values(inplace=True)\n",
    "ave_size.plot(kind='bar', ax=ax[0])\n",
    "ave_size.plot(kind='bar', ax=ax[1])\n",
    "ax[1].set_yscale('log')\n",
    "ave = data['size'].sum() / float(len(data)) / (1024 ** 3)\n",
    "ax[0].hlines(ave, *ax[0].get_xlim())\n",
    "ax[1].hlines(ave, *ax[0].get_xlim())\n",
    "print(\"mean dataset size = %.2f Gb\" % ave)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "grouped_age_days = data.groupby([\"owner\"])[\"age_days\"].mean()\n",
    "grouped_age_days.sort(inplace=True)\n",
    "grouped_age_days.plot(kind='bar', ax=ax)\n",
    "ax.hlines(np.mean(data['age_days']), *ax.get_xlim())\n",
    "ax.set_ylabel('age [days]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average last access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "grouped_lastacc_days = data.groupby([\"owner\"])[\"last_accessed_days\"].mean()\n",
    "grouped_lastacc_days.sort(inplace=True)\n",
    "grouped_lastacc_days.plot(kind='bar', ax=ax)\n",
    "ax.hlines(np.mean(data['last_accessed_days']), *ax.get_xlim())\n",
    "ax.set_ylabel('last accessed [days]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average life"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 6))\n",
    "grouped_life_days = data.groupby([\"owner\"])[\"life_days\"].mean()\n",
    "grouped_life_days.sort(inplace=True)\n",
    "grouped_life_days.plot(kind='bar', ax=ax)\n",
    "ax.hlines(np.mean(data['life_days']), *ax.get_xlim())\n",
    "ax.set_ylabel('last acc - age [days]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cufflinks as cf\n",
    "cf.go_offline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.io.pytables import HDFStore\n",
    "store = HDFStore('store.h5')\n",
    "print(store)\n",
    "data = []\n",
    "for k in store.keys():\n",
    "    try:\n",
    "        d = store.get(k)\n",
    "        d['timestamp'] = pd.to_datetime(k.split(\"_\")[1], format='%d%m%Y')\n",
    "        data.append(d)\n",
    "    except Exception as e:\n",
    "        print(\"Problem reading\", k)\n",
    "        print(e)\n",
    "store.close()        \n",
    "print(len(data))\n",
    "data = pd.concat(data)\n",
    "data = data.set_index(['timestamp', 'owner'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "class NumpyEncoder(json.JSONEncoder):\n",
    "\n",
    "    def default(self, obj):\n",
    "        \"\"\"If input object is an ndarray it will be converted into a dict \n",
    "        holding dtype, shape and the data, base64 encoded.\n",
    "        \"\"\"\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        # Let the base class default method raise the TypeError\n",
    "        return json.JSONEncoder(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = data['size'].unstack().fillna(0)\n",
    "dataplot = data_to_plot.iplot(kind='area', fill=True, asFigure=True)\n",
    "for d in dataplot['data']:\n",
    "    d['hoverinfo'] = 'text+x+name'\n",
    "    d['text'] = [\"%.2f Gb\" % xx for xx in data_to_plot[d['name']].tolist()]\n",
    "data.iplot(data=dataplot['data'])\n",
    "f_json_data = open('data.json', 'w')\n",
    "json_data = json.dump(dataplot['data'], f_json_data, cls=NumpyEncoder)\n",
    "f_json_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['size'].unstack().fillna(0).transpose().to_json(orient='split', date_format='iso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_plot = data['size'].unstack().fillna(0)\n",
    "dataplot = data_to_plot.iplot(kind='scatter', fill=True, asFigure=True)\n",
    "data.iplot(data=dataplot['data'])\n",
    "\n",
    "f_json_data = open('data_scatter.json', 'w')\n",
    "json_data = json.dump(dataplot['data'], f_json_data, cls=NumpyEncoder)\n",
    "f_json_data.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_data = data[data.index.get_level_values('timestamp') == data.index.get_level_values('timestamp').max()]\n",
    "dataplot = latest_data.reset_index().iplot(kind='pie', labels='owner', values='size', hole='0.4', sort=True, textinfo='percent', asFigure=True)\n",
    "dataplot['data'][0]['text'] = [\"%.2f Gb\" % xx for xx in dataplot['data'][0]['values']]\n",
    "dataplot['data'][0]['hoverinfo'] = 'text+label'\n",
    "data.iplot(data=dataplot['data'])\n",
    "\n",
    "f_json_data = open('data_pie.json', 'w')\n",
    "json_data = json.dump(dataplot['data'], f_json_data, cls=NumpyEncoder)\n",
    "f_json_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambda_python3",
   "language": "python",
   "name": "mambda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
